{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a93ca6",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using TorchServe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446701de",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "With Amazon SageMaker multi-model endpoints, customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of many models, which can be served from a common inference container, needs to be called on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the loading and unloading of models for a multi-model endpoint, as they are needed. When an invocation request is made for a particular model, Amazon SageMaker routes the request to an instance assigned to that model, downloads the model artifacts from S3 onto that instance, and initiates loading of the model into the memory of the container. As soon as the loading is complete, Amazon SageMaker performs the requested invocation and returns the result. If the model is already loaded in memory on the selected instance, the downloading and loading steps are skipped, and the invocation is performed immediately.\n",
    "\n",
    "This notebook uses SageMaker notebook instance conda_python3 kernel, demonstrates how to use TorchServe on SageMaker MME. In this example, there are 2 distinct models, each with its own set of dependencies, handler implementation and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc665186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sagemaker pip --upgrade  --quiet\n",
    "!pip install numpy\n",
    "!pip install pillow\n",
    "!pip install -U sagemaker\n",
    "!pip install -U boto \n",
    "!pip install -U botocore\n",
    "!pip install -U boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import sagemaker\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.model import Model\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "region = sess.region_name\n",
    "account = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "smsess = sagemaker.Session(boto_session=sess)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Configuration:\n",
    "bucket_name = smsess.default_bucket()\n",
    "prefix = \"torchserve\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix}/mme/llama-tgz\"\n",
    "print(f\"account={account}, region={region}, role={role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c4028",
   "metadata": {},
   "source": [
    "## Create Model Artifacts\n",
    "This example creates a TorchServe model artifact for each model.\n",
    "### Install torch-model-archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ced165",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-model-archiver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16063ea",
   "metadata": {},
   "source": [
    "### Model 1: meta-llama/Llama-2-7b-chat-hf\n",
    "#### Create Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5332f0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import logging\r\n",
      "from abc import ABC\r\n",
      "from threading import Thread\r\n",
      "\r\n",
      "import torch\r\n",
      "import transformers\r\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\r\n",
      "\r\n",
      "from ts.context import Context\r\n",
      "from ts.handler_utils.hf_batch_streamer import TextIteratorStreamerBatch\r\n",
      "from ts.handler_utils.micro_batching import MicroBatching\r\n",
      "from ts.protocol.otf_message_handler import send_intermediate_predict_response\r\n",
      "from ts.torch_handler.base_handler import BaseHandler\r\n",
      "\r\n",
      "logger = logging.getLogger(__name__)\r\n",
      "logger.info(\"Transformers version %s\", transformers.__version__)\r\n",
      "\r\n",
      "\r\n",
      "class LlamaHandler(BaseHandler, ABC):\r\n",
      "    \"\"\"\r\n",
      "    Transformers handler class for sequence, token classification and question answering.\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    def __init__(self):\r\n",
      "        super(LlamaHandler, self).__init__()\r\n",
      "        self.max_length = None\r\n",
      "        self.max_new_tokens = None\r\n",
      "        self.tokenizer = None\r\n",
      "        self.initialized = False\r\n",
      "        self.output_streamer = None\r\n",
      "        # enable micro batching\r\n",
      "        self.handle = MicroBatching(self)\r\n",
      "\r\n",
      "    def initialize(self, ctx: Context):\r\n",
      "        \"\"\"In this initialize function, the HF large model is loaded and\r\n",
      "        partitioned using DeepSpeed.\r\n",
      "        Args:\r\n",
      "            ctx (context): It is a JSON Object containing information\r\n",
      "            pertaining to the model artifacts parameters.\r\n",
      "        \"\"\"\r\n",
      "        model_dir = ctx.system_properties.get(\"model_dir\")\r\n",
      "        self.max_length = int(ctx.model_yaml_config[\"handler\"][\"max_length\"])\r\n",
      "        self.max_new_tokens = int(ctx.model_yaml_config[\"handler\"][\"max_new_tokens\"])\r\n",
      "        model_name = ctx.model_yaml_config[\"handler\"][\"model_name\"]\r\n",
      "        model_path = f'{model_dir}/{ctx.model_yaml_config[\"handler\"][\"model_path\"]}'\r\n",
      "        seed = int(ctx.model_yaml_config[\"handler\"][\"manual_seed\"])\r\n",
      "        torch.manual_seed(seed)\r\n",
      "\r\n",
      "        # micro batching initialization\r\n",
      "        micro_batching_parallelism = ctx.model_yaml_config.get(\r\n",
      "            \"micro_batching\", {}\r\n",
      "        ).get(\"parallelism\", None)\r\n",
      "        if micro_batching_parallelism:\r\n",
      "            logger.info(\r\n",
      "                f\"Setting micro batching parallelism  from model_config_yaml: {micro_batching_parallelism}\"\r\n",
      "            )\r\n",
      "            self.handle.parallelism = micro_batching_parallelism\r\n",
      "\r\n",
      "        micro_batch_size = ctx.model_yaml_config.get(\"micro_batching\", {}).get(\r\n",
      "            \"micro_batch_size\", 1\r\n",
      "        )\r\n",
      "        logger.info(f\"Setting micro batching size: {micro_batch_size}\")\r\n",
      "        self.handle.micro_batch_size = micro_batch_size\r\n",
      "\r\n",
      "        logger.info(\"Model %s loading tokenizer\", ctx.model_name)\r\n",
      "        self.model = AutoModelForCausalLM.from_pretrained(\r\n",
      "            model_path,\r\n",
      "            device_map=\"balanced\",\r\n",
      "            low_cpu_mem_usage=True,\r\n",
      "            torch_dtype=torch.float16,\r\n",
      "            load_in_8bit=True,\r\n",
      "            trust_remote_code=True,\r\n",
      "        )\r\n",
      "        if ctx.model_yaml_config[\"handler\"][\"fast_kernels\"]:\r\n",
      "            from optimum.bettertransformer import BetterTransformer\r\n",
      "\r\n",
      "            try:\r\n",
      "                self.model = BetterTransformer.transform(self.model)\r\n",
      "            except RuntimeError as error:\r\n",
      "                logger.warning(\r\n",
      "                    \"HuggingFace Optimum is not supporting this model,for the list of supported models, please refer to this doc,https://huggingface.co/docs/optimum/bettertransformer/overview\"\r\n",
      "                )\r\n",
      "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n",
      "        self.tokenizer.add_special_tokens(\r\n",
      "            {\r\n",
      "                \"pad_token\": \"<PAD>\",\r\n",
      "            }\r\n",
      "        )\r\n",
      "        self.model.resize_token_embeddings(self.model.config.vocab_size + 1)\r\n",
      "\r\n",
      "        self.output_streamer = TextIteratorStreamerBatch(\r\n",
      "            self.tokenizer,\r\n",
      "            batch_size=self.handle.micro_batch_size,\r\n",
      "            skip_special_tokens=True,\r\n",
      "        )\r\n",
      "\r\n",
      "        logger.info(\"Model %s loaded successfully\", ctx.model_name)\r\n",
      "        self.initialized = True\r\n",
      "\r\n",
      "    def preprocess(self, requests):\r\n",
      "        \"\"\"\r\n",
      "        Basic text preprocessing, based on the user's choice of application mode.\r\n",
      "        Args:\r\n",
      "            requests (list): A list of dictionaries with a \"data\" or \"body\" field, each\r\n",
      "                            containing the input text to be processed.\r\n",
      "        Returns:\r\n",
      "            tuple: A tuple with two tensors: the batch of input ids and the batch of\r\n",
      "                attention masks.\r\n",
      "        \"\"\"\r\n",
      "        input_text = []\r\n",
      "        for req in requests:\r\n",
      "            data = req.get(\"data\") or req.get(\"body\")\r\n",
      "            if isinstance(data, (bytes, bytearray)):\r\n",
      "                data = data.decode(\"utf-8\")\r\n",
      "\r\n",
      "            logger.info(f\"received req={data}\")\r\n",
      "            input_text.append(data.strip())\r\n",
      "\r\n",
      "        # Ensure the compiled model can handle the input received\r\n",
      "        if len(input_text) > self.handle.micro_batch_size:\r\n",
      "            raise ValueError(\r\n",
      "                f\"Model is compiled for batch size {self.handle.micro_batch_size} but received input of size {len(input_ids_batch)}\"\r\n",
      "            )\r\n",
      "\r\n",
      "        # Pad input to match compiled model batch size\r\n",
      "        input_text.extend([\"\"] * (self.handle.micro_batch_size - len(input_text)))\r\n",
      "\r\n",
      "        return self.tokenizer(input_text, return_tensors=\"pt\", padding=True)\r\n",
      "\r\n",
      "    def inference(self, input_batch):\r\n",
      "        \"\"\"\r\n",
      "        Predicts the class (or classes) of the received text using the serialized transformers\r\n",
      "        checkpoint.\r\n",
      "        Args:\r\n",
      "            input_batch (tuple): A tuple with two tensors: the batch of input ids and the batch\r\n",
      "                                of attention masks, as returned by the preprocess function.\r\n",
      "        Returns:\r\n",
      "            list: A list of strings with the predicted values for each input text in the batch.\r\n",
      "        \"\"\"\r\n",
      "        generation_kwargs = dict(\r\n",
      "            input_batch,\r\n",
      "            max_new_tokens=self.max_new_tokens,\r\n",
      "            streamer=self.output_streamer,\r\n",
      "        )\r\n",
      "\r\n",
      "        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\r\n",
      "        thread.start()\r\n",
      "\r\n",
      "        micro_batch_idx = self.handle.get_micro_batch_idx()\r\n",
      "        micro_batch_req_id_map = self.get_micro_batch_req_id_map(micro_batch_idx)\r\n",
      "        for new_text in self.output_streamer:\r\n",
      "            logger.debug(\"send response stream\")\r\n",
      "            send_intermediate_predict_response(\r\n",
      "                new_text[: len(micro_batch_req_id_map)],\r\n",
      "                micro_batch_req_id_map,\r\n",
      "                \"Intermediate Prediction success\",\r\n",
      "                200,\r\n",
      "                self.context,\r\n",
      "            )\r\n",
      "\r\n",
      "        thread.join()\r\n",
      "\r\n",
      "        return [\"\"] * len(micro_batch_req_id_map)\r\n",
      "\r\n",
      "    def postprocess(self, inference_output):\r\n",
      "        \"\"\"Post Process Function converts the predicted response into Torchserve readable format.\r\n",
      "        Args:\r\n",
      "            inference_output (list): It contains the predicted response of the input text.\r\n",
      "        Returns:\r\n",
      "            (list): Returns a list of the Predictions and Explanations.\r\n",
      "        \"\"\"\r\n",
      "        return inference_output\r\n",
      "\r\n",
      "    def get_micro_batch_req_id_map(self, micro_batch_idx: int):\r\n",
      "        start_idx = micro_batch_idx * self.handle.micro_batch_size\r\n",
      "        micro_batch_req_id_map = {\r\n",
      "            index: self.context.request_ids[batch_index]\r\n",
      "            for index, batch_index in enumerate(\r\n",
      "                range(start_idx, start_idx + self.handle.micro_batch_size)\r\n",
      "            )\r\n",
      "            if batch_index in self.context.request_ids\r\n",
      "        }\r\n",
      "\r\n",
      "        return micro_batch_req_id_map\r\n"
     ]
    }
   ],
   "source": [
    "# Implement customized handler\n",
    "!cat workspace/llama2-7b-chat/custom_handler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a28f3e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TorchServe frontend parameters\r\n",
      "minWorkers: 1\r\n",
      "maxWorkers: 1\r\n",
      "maxBatchDelay: 100\r\n",
      "responseTimeout: 1200\r\n",
      "deviceType: \"gpu\"\r\n",
      "\r\n",
      "handler:\r\n",
      "    model_name: \"meta-llama/Llama-2-7b-chat-hf\"\r\n",
      "    model_path: \"model/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235\"\r\n",
      "    max_length: 50\r\n",
      "    max_new_tokens: 50\r\n",
      "    manual_seed: 40\r\n",
      "    fast_kernels: True\r\n",
      "\r\n",
      "micro_batching:\r\n",
      "    micro_batch_size: 4\r\n",
      "    parallelism:\r\n",
      "        preprocess: 2\r\n",
      "        inference: 1\r\n",
      "        postprocess: 2\r\n"
     ]
    }
   ],
   "source": [
    "# Config model-config.yaml\n",
    "!cat workspace/llama2-7b-chat/model-config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cd071af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.31.0\r\n",
      "accelerate\r\n",
      "bitsandbytes\r\n",
      "scipy\r\n",
      "mpi4py\r\n",
      "optimum\r\n"
     ]
    }
   ],
   "source": [
    "# 3rd party dependencies\n",
    "!cat workspace/llama2-7b-chat/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efea4de",
   "metadata": {},
   "source": [
    "#### Download meta-llama/Llama-2-7b-chat-hf (See [details](https://github.com/pytorch/serve/blob/a41686f9801f4f12df31c7fee779b51acd1c3b32/examples/large_models/Huggingface_accelerate/llama2/Readme.md?plain=1#L5) in TorchServe example)  and Move to model artifacts\n",
    "\n",
    "**Note**: remove symbolic link in model since S3 does not allow symbolic link\n",
    "\n",
    "For example: meta-llama/Llama-2-7b-chat-hf is downloaded in folder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11924275",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv model llama2-7b-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3491e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model artifacts using torch-model-archiver\n",
    "!torch-model-archiver --model-name llama2-7b-chat-int8-no-req --version 1.0 --handler workspace/llama2-7b-chat/custom_handler.py --config-file workspace/llama2-7b-chat/model-config.yaml -r workspace/llama2-7b-chat/requirements.txt --archive-format tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b96f37",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp llama2-7b-chat-int8-no-req.tar.gz {output_path}/llama2-7b-chat-int8-no-req.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88840dd0",
   "metadata": {},
   "source": [
    "### Model 2: meta-llama/Llama-2-7b-hf\n",
    "#### Create Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement customized handler\n",
    "!cat workspace/llama2-7b/custom_handler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config model-config.yaml\n",
    "!cat workspace/llama2-7b/model-config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300914db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd party dependencies\n",
    "!cat workspace/llama2-7b/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914d0c2",
   "metadata": {},
   "source": [
    "#### Download meta-llama/Llama-2-7b-hf (See [details](https://github.com/pytorch/serve/blob/a41686f9801f4f12df31c7fee779b51acd1c3b32/examples/large_models/Huggingface_accelerate/llama2/Readme.md?plain=1#L5) in TorchServe example)  and Move to model artifacts\n",
    "\n",
    "**Note**: remove symbolic link in model since S3 does not allow symbolic link\n",
    "\n",
    "For example: meta-llama/Llama-2-7b-hf is downloaded in folder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv model llama2-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fecb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model artifacts using torch-model-archiver\n",
    "!torch-model-archiver --model-name llama2-7b-int8-no-req --version 1.0 --handler workspace/llama2-7b/custom_handler.py --config-file workspace/llama2-7b/model-config.yaml -r workspace/llama2-7b/requirements.txt --archive-format tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c1a4e",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp llama2-7b-int8-no-req.tar.gz {output_path}/llama2-7b-int8-no-req.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd33e1",
   "metadata": {},
   "source": [
    "## Create the Multi-Model Endpoint with the SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c9f75",
   "metadata": {},
   "source": [
    "### Create the Amazon SageMaker MultiDataModel entity\n",
    "\n",
    "We create the multi-model endpoint using the [```MultiDataModel```](https://sagemaker.readthedocs.io/en/stable/api/inference/multi_data_model.html) class.\n",
    "\n",
    "You can create a MultiDataModel by directly passing in a `sagemaker.model.Model` object - in which case, the Endpoint will inherit information about the image to use, as well as any environmental variables, network isolation, etc., once the MultiDataModel is deployed.\n",
    "\n",
    "In addition, a MultiDataModel can also be created without explicitly passing a `sagemaker.model.Model` object. Please refer to the documentation for additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f50efe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-084495728311/torchserve/mme/llama-tgz\n",
      "<sagemaker.multidatamodel.MultiDataModel object at 0x7f714a6cb160>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "# This is where our MME will read models from on S3.\n",
    "multi_model_s3uri = output_path\n",
    "print(multi_model_s3uri)\n",
    "model = Model(\n",
    "    model_data=f\"{multi_model_s3uri}/llama2-7b-chat-int8-no-req.tar.gz\",\n",
    "    #image_uri=container,\n",
    "    image_uri=\"084495728311.dkr.ecr.us-west-2.amazonaws.com/torchserve-mme-demo:0.9.0.5\",\n",
    "    role=role,\n",
    "    sagemaker_session=smsess,\n",
    "    env={\n",
    "        \"TS_INSTALL_PY_DEP_PER_MODEL\": \"true\",\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": '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',\n",
    "        \"HF_DATASETS_CACHE\": '/tmp/.cache/huggingface',\n",
    "    },\n",
    ")\n",
    "\n",
    "mme = MultiDataModel(\n",
    "    name=\"torchserve-mme-llama-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "    model_data_prefix=multi_model_s3uri,\n",
    "    model=model,\n",
    "    sagemaker_session=smsess,\n",
    ")\n",
    "print(mme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abb202",
   "metadata": {},
   "source": [
    "### Deploy the Multi-Model Endpoint\n",
    "\n",
    "You need to consider the appropriate instance type and number of instances for the projected prediction workload across all the models you plan to host behind your multi-model endpoint. The number and size of the individual models will also drive memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a86e8be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "    print(\"Deleting previous endpoint...\")\n",
    "    time.sleep(10)\n",
    "except (NameError, ClientError):\n",
    "    pass\n",
    "\n",
    "mme.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    model_data_download_timeout=3600,\n",
    "    volume_size=512,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8e6dc",
   "metadata": {},
   "source": [
    "### Our endpoint has launched! Let's look at what models are available to the endpoint!\n",
    "\n",
    "By 'available', what we mean is, what model artifacts are currently stored under the S3 prefix we defined when setting up the `MultiDataModel` above i.e. `model_data_prefix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a174381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/', '/llama2-7b-chat-int8-no-req.tar.gz']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only sam.tar.gz visible!\n",
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ae3e9",
   "metadata": {},
   "source": [
    "### Dynamically deploying models to the endpoint\n",
    "\n",
    "The `.add_model()` method of the `MultiDataModel` will copy over our model artifacts from where they were initially stored, by training, to where our endpoint will source model artifacts for inference requests.\n",
    "\n",
    "Note that we can continue using this method, as shown below, to dynamically deploy more models to our live endpoint as required!\n",
    "\n",
    "`model_data_source` refers to the location of our model artifact (i.e. where it was deposited on S3 after training completed)\n",
    "\n",
    "`model_data_path` is the **relative** path to the S3 prefix we specified above (i.e. `model_data_prefix`) where our endpoint will source models for inference requests. Since this is a **relative** path, we can simply pass the name of what we wish to call the model artifact at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52079e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"llama2-7b-int8-no-req.tar.gz\", \n",
    "    \"llama2-7b-int8-no-req-1.tar.gz\", \n",
    "    \"llama2-7b-int8-no-req-2.tar.gz\", \n",
    "    \"llama2-7b-int8-no-req-3.tar.gz\", \n",
    "    \"llama2-7b-chat-int8-no-req-1.tar.gz\", \n",
    "    \"llama2-7b-chat-int8-no-req-2.tar.gz\", \n",
    "    \"llama2-7b-chat-int8-no-req-3.tar.gz\", \n",
    "    \"codellama-7b.tar.gz\", \n",
    "    \"llama2-7b-fp16-no-req.tar.gz\"]\n",
    "for model in models:\n",
    "    mme.add_model(model_data_source=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1470f7",
   "metadata": {},
   "source": [
    "### Our models are ready to invoke!\n",
    "\n",
    "We can see that the S3 prefix we specified when setting up `MultiDataModel` now has model artifacts listed. As such, the endpoint can now serve up inference requests for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26261ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '/codellama-7b.tar.gz',\n",
       " '/llama2-7b-chat-int8-no-req-1.tar.gz',\n",
       " '/llama2-7b-chat-int8-no-req-2.tar.gz',\n",
       " '/llama2-7b-chat-int8-no-req-3.tar.gz',\n",
       " '/llama2-7b-chat-int8-no-req.tar.gz',\n",
       " '/llama2-7b-fp16-no-req.tar.gz',\n",
       " '/llama2-7b-int8-1.tar.gz',\n",
       " '/llama2-7b-int8-2.tar.gz',\n",
       " '/llama2-7b-int8-3.tar.gz',\n",
       " '/llama2-7b-int8-no-req.tar.gz']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d28e7",
   "metadata": {},
   "source": [
    "## Get predictions from the endpoint\n",
    "\n",
    "Recall that `mme.deploy()` returns a [Real Time Predictor](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/predictor.py#L35) that we saved in a variable called `predictor`.\n",
    "\n",
    "That `predictor` can now be used as usual to request inference - but specifying which model to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f24a0bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor: {'endpoint_name': 'torchserve-mme-llama-2023-10-14-00-55-26', 'sagemaker_session': <sagemaker.session.Session object at 0x7f73bf2e8f40>, 'serializer': <sagemaker.base_serializers.IdentitySerializer object at 0x7f714d328070>, 'deserializer': <sagemaker.base_deserializers.BytesDeserializer object at 0x7f714d328370>}\n"
     ]
    }
   ],
   "source": [
    "predictor = sagemaker.predictor.Predictor(endpoint_name=mme.endpoint_name, sagemaker_session=smsess)\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ea4e5",
   "metadata": {},
   "source": [
    "### Model meta-llama/Llama-2-7b-hf Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5008ebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": \"Hey, are you conscious? Can you talk to me?\"}\n",
      "\n",
      "{\"data\": \"I'm not sure. I'm not sure I can talk to you.\"}\n",
      "\n",
      "{\"data\": \"I\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"data\": \"Hey, are you conscious? Can you talk to me?\"\n",
    "}).encode('utf-8')\n",
    "\n",
    "response = predictor.predict(data=payload, target_model=\"/llama2-7b-int8-no-req.tar.gz\").decode('utf-8')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862591a0",
   "metadata": {},
   "source": [
    "### Model meta-llama/Llama-2-7b-chat-hf Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa8d8427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": \"Hey, are you conscious? Can you talk to me?\"}\n",
      "\n",
      "This is an example of a text message that could be sent to a chatbot or virtual assistant, such as Amazon's Alexa or Google\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"data\": \"Hey, are you conscious? Can you talk to me?\"\n",
    "}).encode('utf-8')\n",
    "\n",
    "response = predictor.predict(data=payload, target_model=\"/llama2-7b-chat-int8-no-req.tar.gz\").decode('utf-8')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880e078",
   "metadata": {},
   "source": [
    "## Updating a model\n",
    "\n",
    "To update a model, you would follow the same approach as above and add it as a new model. For example, `ModelA-2`.\n",
    "\n",
    "You should avoid overwriting model artifacts in Amazon S3, because the old version of the model might still be loaded in the endpoint's running container(s) or on the storage volume of instances on the endpoint: This would lead invocations to still use the old version of the model.\n",
    "\n",
    "Alternatively, you could stop the endpoint and re-deploy a fresh set of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb05ed2",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Endpoints should be deleted when no longer in use, since (per the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/)) they're billed by time deployed. Here we'll also delete the endpoint configuration - to keep things tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
