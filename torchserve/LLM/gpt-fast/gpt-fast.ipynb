{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0524d4b4",
   "metadata": {},
   "source": [
    "# PyTorch Native Transformer Text Gen Using TorchServe on SageMaker with Llama-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a529f6",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[GPT fast](https://github.com/pytorch-labs/gpt-fast) is a simple and efficient pytorch-native transformer text generation.\n",
    "\n",
    "It features:\n",
    "* Very low latency\n",
    "* <1000 lines of python\n",
    "* No dependencies other than PyTorch and sentencepiece\n",
    "* int8/int4 quantization\n",
    "* Speculative decoding\n",
    "* Tensor parallelism\n",
    "* Supports Nvidia and AMD GPUs\n",
    "\n",
    "More details about gpt-fast can be found in this [blog](https://pytorch.org/blog/accelerating-generative-ai-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19ac44",
   "metadata": {},
   "source": [
    "## Step 1: Let's bump up SageMaker and import stuff\n",
    "\n",
    "The wheel installed here is a private preview wheel, you need to add into allowlist to run this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6901ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ab917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest aws cli v2 if it is not installed\n",
    "!curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "!unzip awscliv2.zip\n",
    "#Follow the instruction to install aws v2 on the terminal\n",
    "!cat aws/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f63ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conform it is aws-cli/2.xx.xx\n",
    "!aws --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d68be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install sagemaker pip --upgrade  --quiet\n",
    "!pip install numpy\n",
    "!pip install pillow\n",
    "!pip install -U sagemaker\n",
    "!pip install -U boto \n",
    "!pip install -U botocore\n",
    "!pip install -U boto3\n",
    "!pip install torch-model-archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874a3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "boto3_session=boto3.session.Session(region_name=\"us-west-2\")\n",
    "smr = boto3.client('sagemaker-runtime-demo')\n",
    "sm = boto3.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess= sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "# Configuration:\n",
    "bucket_name = sess.default_bucket()\n",
    "prefix = \"torchserve\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix}\"\n",
    "print(f'account={account}, region={region}, role={role}, output_path={output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c2c9a",
   "metadata": {},
   "source": [
    "## Step 2: Build a BYOD TorchServe Docker container and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a00114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install our own dependencies\n",
    "!cat workspace/docker/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c82d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture build_output\n",
    "\n",
    "baseimage = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.1.0-gpu-py310-cu118-ubuntu20.04-sagemaker\"\n",
    "reponame = \"torchserve-gpt-fast\"\n",
    "versiontag = \"0.1\"\n",
    "\n",
    "# Build our own docker image\n",
    "!cd workspace/docker && ./build_and_push.sh {reponame} {versiontag} {baseimage} {region} {account}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c84373",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Error response from daemon' in str(build_output):\n",
    "    print(build_output)\n",
    "    raise SystemExit('\\n\\n!!There was an error with the container build!!')\n",
    "else:\n",
    "    container = str(build_output).strip().split('\\n')[-1]\n",
    "\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Build model artifacts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b45e0e6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-03T23:03:15.245363Z",
     "end_time": "2023-12-03T23:03:15.398619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#frontend settings\r\n",
      "minWorkers: 1\r\n",
      "maxWorkers: 1\r\n",
      "maxBatchDelay: 200\r\n",
      "responseTimeout: 10800\r\n",
      "parallelType: \"tp\"\r\n",
      "deviceType: \"gpu\"\r\n",
      "continuousBatching: false\r\n",
      "\r\n",
      "handler:\r\n",
      "  model_name: \"meta-llama/Llama-2-70b-chat-hf\"\r\n",
      "  converted_ckpt_dir: \"checkpoints/meta-llama/Llama-2-70b-chat-hf/model.pth\"\r\n",
      "  draft_model_name: \"meta-llama/Llama-2-7b-chat-hf\"\r\n",
      "  draft_checkpoint_dir: \"checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"\r\n",
      "  quantization: int8\r\n",
      "  max_new_tokens: 50\r\n",
      "  compile: true\r\n",
      "  speculate_k: 8"
     ]
    }
   ],
   "source": [
    "# model configuration\n",
    "!cat workspace/model-config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create model artifact folder\n",
    "!torch-model-archiver --model-name gpt-fast --version 1.0 --handler handler.py --config-file model-config.yaml --archive-format no-archive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "fd48ab5d",
   "metadata": {},
   "source": [
    "## Step 4: Upload model artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp gpt-fast {output_path}/gpt-fast --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa339407",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri = f\"{output_path}/gpt-fast/\"\n",
    "print(s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c259c",
   "metadata": {},
   "source": [
    "## Step 5: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771053b",
   "metadata": {},
   "source": [
    "### Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bcfdfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model.Model object at 0x7fda64bf7040>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "instance_type = \"ml.inf2.24xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"ts-gpt-fast\")\n",
    "hf_token = \"YOUR_TOKEN\"\n",
    "\n",
    "model = Model(\n",
    "    name=\"torchserve-gpt-fast\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "    # Enable SageMaker uncompressed model artifacts\n",
    "    model_data={\n",
    "        \"S3DataSource\": {\n",
    "                \"S3Uri\": s3_uri,\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"CompressionType\": \"None\",\n",
    "        }\n",
    "    },\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    env={\"HUGGING_FACE_HUB_TOKEN\": hf_token,},\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53eaa416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    volume_size=512, # increase the size to store large model\n",
    "    model_data_download_timeout=3600, # increase the timeout to download large model\n",
    "    container_startup_health_check_timeout=600, # increase the timeout to load large model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39513f65",
   "metadata": {},
   "source": [
    "## Run the inference with streaming response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89453f2",
   "metadata": {},
   "source": [
    "### SageMaker streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "360839a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class Parser:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        data = self.buff.getvalue()\n",
    "        \n",
    "    def scan_lines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f760056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on going to the beach. I am going to take my camera and take some pictures of the beach. I am going to take pictures of the sand, the water, and the people. I am also going to take pictures of the sunset. I am really excited to go to the beach and take pictures. The beach is a great place to take pictures. The sand, the water, and the people are all great subjects for pictures. The sunset is also a great subject for pictures "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "prompt = \"The capital of France\".encode('utf-8')\n",
    "body = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_new_tokens\": 50,\n",
    "}\n",
    "resp = smr.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=body, ContentType=\"application/json\")\n",
    "event_stream = resp['Body']\n",
    "parser = Parser()\n",
    "for event in event_stream:\n",
    "    parser.write(event['PayloadPart']['Bytes'])\n",
    "    for line in parser.scan_lines():\n",
    "        print(line.decode(\"utf-8\"), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2e7b4",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bcaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
